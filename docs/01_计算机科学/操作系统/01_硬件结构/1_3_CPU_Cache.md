![CPU Cache](cpu_cache_01.png)
/// caption
CPU Cache
///


## 为什么存在 CPU Cache?

CPU的访问速度增长非常迅速，每 18 个月左右就会翻倍，而内存速度增长没有它那么快。随着两者速度之间差异越来越大，CPU 访问内存逐渐被认为是一个慢速的过程。CPU Cache 在两者之间扮演了高速缓存的角色，CPU 要访问某个内存数据的时候，会依次去 L1、L2、L3 三级高速缓存中找，找到则直接返回，否则最终还是会访问内存去拿。因为数据存在局部性特征，CPU Cache 的命中率其实还可以，一定程度上弥补了 CPU 访问内存的性能问题。

CPU 访问 CPU Cache 和内存的时间对比：

| Target      | Time Required          |
| ----------- | ---------------------- |
| L1 Cache    | 2 ~ 4 clock cycles     |
| L2 Cache    | 10 ~ 20 clock cycles   |
| L3 Cache    | 20 ~ 60 clock cycles   |
| Memory      | 200 ~ 300 clock cycles |


## CPU Cache 的结构

CPU Cache 按照单个块来读取数据，单个这样的块称为一个 Cache Line 。

现代处理器的 Cache Line 的大小一般在 64 个字节左右，某些老的处理器是 32 个字节，一些高性能的处理器可以达到 128 字节。这里我们说的 Cache Line 的大小指的是它实际存储数据的容量，其实它还有一些额外组成部分，如下：

| 组成部分 | 大小 | 作用 |
| --- | --- | --- |
| Valid Bit | 1 位 | 是否有效 |
| Dirty Bit | 1 位 | 是否被修改过 |
| Tag | 多位 | 地址标记，用于匹配 |
| Data Block | 64 字节 | 存储数据 |

### CPU Cache 如何写入和读取数据？

按照存储器多级层次的规则，CPU 在访问内存数据的时候，并不是跳过 CPU Cache 直接读取内存数据。内存数据首先被复制写入相邻层次的 L3，然后是 L2、L1，最后写入寄存器。

#### 1.边界对齐

数据加载进 Cache Line 中时，要严格对齐 64 字节的边界，怎么理解呢？假设我们有一个长度为 100 的 C# int 数组 array ，在 C# 中单个 int 占用 4 字节。当 CPU 读取 `array[0]` 时，由于只占用了 4 个字节，不满 64 字节，所以需要把目标地址之后的连续数据读入，直到占满 64 字节。所以 `array[0] ~ array[15]` 被一起加载进 Cache Line 。**数据局部性**一部分原因也在于此，相邻内存地址的数据往往存在于同一个 Cache Line 中，所以访问速度很快。

!!! note "非数组的连续地址，Cache Line 怎么加载的"
    在上面的例子中，我们举例的是一个数组。那么非数组的情况呢？加入这段地址中全是没有关联的若干数据，如何加载？其实和数据的存在形式无关，写入 Cache Line 的就是这段连续地址的数据，如果连续地址中途遇到了内存空洞，即没有数据的情况，现代 CPU 也设计了跳过空洞地址的策略，数据最后会对齐 64 字节的边界，然后写入 Cache Line

#### 2.内存地址映射

内存空间的地址范围是远大于 CPU Cache 的地址范围的，因此数据从内存中加载进 CPU Cache 时，需要做一次地址映射，采用某种计算将数据在内存上的物理地址转换为在 CPU Cache 上的物理地址。映射的策略很多，我们介绍最简单的**直接映射 Cache（Direct Mapped Cache）**，其他还有全相连（Fully Associative Cache）和组相连（Set Associative Cache）等。

直接映射的实现其实就是**取模运算**，假设现在一共有 8 个 Cache Line 。那么加载第 15 个内存块，最终会被放入 7 号 Cache Line 中。

#### 3.从 Cache Line 读取数据

上面内存地址直接映射其实有一个问题没有解释，那就是：从内存加载数据的时候，理论上会有多个内存地址的数据被映射在同一个 Cache Line 中，比如用上方例子继续举例，除了第 15 个内存块，第 23、31、39 ... 等等内存块被加载时也会进入 7 号 Cache Line ，这里是一对多的关系。我们知道 CPU 读取目标数据时，会先从 Cache Line 中找，那么 CPU 在读 Cache Line 时，是如何判断其是否是目标内存地址的数据呢？

为了区分 7 号 Cache Line 当前存储的到底是 15 还是 23、31、39 ... 内存块的数据，Cache Line 中还会存储一个**组标记（Tag）**。这个组标记用于 CPU 判断是否是目标内存地址。

第二个问题是，一个 CPU Cache Line 的数据有 64 字节，除了目标数据，其它内存地址的数据也连带被加载进来了，CPU 是如何从中找到目标数据的呢？CPU 从 Cache Line 读取数据时，不会读取整个 Data Block，而是按照单个**“字”（Word）**读取，整个 Data Block 被划分为若干的字，只需要一个偏移量，CPU 就知道到底读哪个字。

所以，一个内存地址被拆分成三个部分，作为 **CPU Cache Line 索引**、**组标记**、**偏移量** 三个部分的值。整合前面的三个问题，我们可以得到 CPU 从 Cache Line 读取数据的全貌：

![从 Cache Line 读取数据](cpu_cache_02.png)
/// caption
从 Cache Line 读取数据
///

总结 CPU 从 Cache Line 读取数据的步骤：

- 从内存地址得到 Cache Line 索引，找到对应的 Cache Line
- 判断有效位，如果无效，CPU 会直接访问内存，并重新加载数据；如果有效，继续往下步执行
- 对比组标记，如果不匹配，CPU 会直接访问内存，并重新加载数据；如果有效，继续往下步执行
- 根据偏移量，读取 Data Block 中对应的字


## 缓存命中率和 Cache Miss

缓存命中率，是指当 CPU 需要从内存中读取数据时，在 CPU Cache 中找到并直接返回的概率。我们知道 CPU Cache 的访问速度是几十上百倍快于直接访问内存的，所以缓存命中率越高，CPU 性能表现也就越好。

与之对应的，存在 Cache Miss 这样一个说法，它的意思是 CPU 读取某个数据时，在 CPU Cache 中没有找到目标数据，需要从内存中读取。Cache Miss 的效率在 CPU 这个层面是非常慢的。

所以我们在开发中，提升缓存命中率减少 Cache Miss 是一个性能优化的手段。我们知道在 L1 层面，CPU Cache 被分为数据缓存和指令缓存，这两种类型的缓存我们都可以通过调整代码来提升它的命中率，下面来看两个例子。

### 1.如何提升数据缓存的命中率？

``` csharp title="对比两种形式的遍历效率" linenums="1" hl_lines="0"
var array = new int[m][n] { .... };

// 形式一
for (var i = 0; i < m; i++)
{
    for (int j = 0; j < n; j++)
    {
        array[i][j] = 0;
    }
}

// 形式二
for (var i = 0; i < m; i++)
{
    for (int j = 0; j < n; j++)
    {
        array[j][i] = 0;
    }
}
```

这个例子中形式一的速度比形式二更快。形式一访问内存地址的顺序，和二维数组 `array` 的数组元素的内存分布顺序是一致的。由于 Cache Line 按照连续的内存地址加载对齐 64 字节的数据，因此数据局部性的利用率很高，Cache Miss 的次数很少。第二种写法，访问的方式是跳跃的，访问某些元素时，由于 Cache Line 中缓存的数据已经被擦除了，导致需要重新去内存加载，产生了更多次 Cache Miss。

因此我们可以得到的结论是，按照数据内存布局顺序访问数据，可以有效利用 CPU Cache 的设计特性，大大减少 Cache Miss，提高数据缓存命中率。

### 2.如何提高指令缓存命中率？

``` csharp title="对比两种形式的执行效率" linenums="1" hl_lines="0"
var array = new int[n];
for (int i = 0; i < n; i++)
{
    array[i] = rand() % 100;
}

// 形式一，先遍历再排序
for (var i = 0; i < n; i++)
{
    if (array[i] < 50)
    {
        array[i] = 0;
    }
}
sort(array);

// 形式二
sort(array);
for (var i = 0; i < n; i++)
{
    if (array[i] < 50)
    {
        array[i] = 0;
    }
}
```

这个例子中，形式二的效率更高。

为什么呢？要解释这个问题，需要知道 CPU 存在**分支预测**的能力，分支预测可以预测接下来是执行 if 指令还是 else，从而提前将指令放在指令缓存中，这样 CPU 加载指令的速度会更快，不用从内存中加载指令。

上面例子中的数组完全是运行时随机填充的，而分支预测器会根据历史命中数据来为未来进行预测，所以分支预测无法有效工作。直接遍历再排序，分支预测器在遍历阶段预测的结果会多次失败导致指令命中率低下。但如果我们先排序，前几次循环会出现命中、命中、...只有在某次条件分支中元素值大于 50 时发生切换，所以命中率会提高。

!!! note "显示使用分支预测"
    C/C++语言编译器中提供了 `likely` 和 `unlikely` 两种宏，用于显示使用分支预测，Lua 的源码中多次使用了这个特性。

!!! tip "题外话：提升多核 CPU 的缓存命中率"
    由于 L1 和 L2 不是全部核心共享的，所以当一个线程在不同的核心之间来回切换时，CPU Cache 的命中率也会受到影响。在执行多个“计算密集型”线程的时候，可以选择把线程绑定在某一个 CPU 核心上，以此来优化缓存命中率。对于 Unity 来说，这个优化策略不常用，因为 Unity 大部分工作内容运行在主线程。